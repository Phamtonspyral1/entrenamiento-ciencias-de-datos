# -*- coding: utf-8 -*-
"""Manipulacion mineria de datos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JHPiE9EXZcpwjaFgtFHCcBlSpVdjplpv
"""

#Importamos las librerias mas comunes
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

file_path = "/datos-hospital-a.xls"
df = pd.read_excel(file_path)
display(df.head())

# Mostrar todas las filas y columnas
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

# Ver el dataset completo
print(df)

df.isnull().sum()

df.notnull().sum()

duplicados = df[df.duplicated()]
print("Número de registros duplicados:", duplicados.shape[0])
if duplicados.shape[0] > 0:
    print("Ejemplos de registros duplicados:")
    display(duplicados.head())

# Seleccionar columnas numéricas para la detección de outliers
df_numeric = df.select_dtypes(include=np.number)

# Crear boxplots para cada columna numérica
plt.figure(figsize=(15, 10))
df_numeric.boxplot()
plt.title('vizualizacion de Variables Numéricas para Detección de Outliers')
plt.ylabel('Valor')
plt.xticks(rotation=25)
plt.grid(True)
plt.show()

"""hicimos un analisis exploratorio de que campos y valores teniamos aqui y buscamos los valores nulos y observamos si tambien habian valores duplicados
en este primer data set utilizamos deteccion y manejo de valores para saber si su consistencia estaba correcta

"""

file_path = "/datos-hospital-b.csv"
df = pd.read_csv(file_path)
display(df.head())

# Mostrar todas las filas y columnas
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

# Ver el dataset completo
print(df)

df.isnull().sum()

df.notnull().sum()

duplicados = df[df.duplicated()]
print("Número de registros duplicados:", duplicados.shape[0])
if duplicados.shape[0] > 0:
    print("\nEjemplos de registros duplicados:")
    display(duplicados.head())

# Seleccionar columnas numéricas para la detección de outliers
df_numeric = df.select_dtypes(include=np.number)

# Crear boxplots para cada columna numérica
plt.figure(figsize=(9, 11))
df_numeric.boxplot()
plt.title(' Variables Numéricas para Detección de Outliers')
plt.ylabel('Valor')
plt.xticks(rotation=25)
plt.grid(True)
plt.show()

# Mostrar valores únicos en la columna 'SEXO' antes de la corrección
print("Valores únicos en 'SEXO' antes de la corrección:", df['SEXO'].unique())

# Estandarizar la columna 'SEXO' (asumiendo 0 para Femenino y 1 para Masculino)
# Puedes ajustar esto según los valores reales si son diferentes
df['SEXO'] = df['SEXO'].astype(str).str.upper().replace({'0': 'FEMENINO', '1': 'MASCULINO'})


# Mostrar valores únicos en la columna 'SEXO' después de la corrección
print("Valores únicos en 'SEXO' después de la corrección:", df['SEXO'].unique())

# Mostrar los tipos de datos de las columnas de fecha antes de la conversión
print("Tipos de datos de las columnas de fecha antes de la conversión:")
print(df[['F_NACIM', 'F_INCLUS']].dtypes)

# Convertir las columnas de fecha a tipo datetime
df['F_NACIM'] = pd.to_datetime(df['F_NACIM'], errors='coerce')
df['F_INCLUS'] = pd.to_datetime(df['F_INCLUS'], errors='coerce')

# Mostrar los tipos de datos de las columnas de fecha después de la conversión
print("Tipos de datos de las columnas de fecha después de la conversión:")
print(df[['F_NACIM', 'F_INCLUS']].dtypes)

# Mostrar las primeras filas para ver los cambios en las columnas corregidas
print("Primeras filas después de la corrección de inconsistencias:")
display(df.head())

"""

Durante el proceso de limpieza del conjunto de datos "datos-hospital a-b.csv", realizamos las siguientes acciones clave:

1.  Carga del Dataset: Cargamos el archivo CSV en un DataFrame de pandas para su procesamiento.
2.  Eliminación de Duplicados: Verificamos la presencia de registros duplicados. Se confirmó que no existían duplicados en este conjunto de datos, por lo que no fue necesaria una eliminación.
3.  Corrección de Datos Inconsistentes:
    *   Estandarización de 'SEXO': Convertimos los valores numéricos (0 y 1) a categorías descriptivas ('FEMENINO' y 'MASCULINO') para mejorar la legibilidad y consistencia.
    *   Conversión de Tipos de Datos de Fecha: Convertimos las columnas 'F_NACIM' y 'F_INCLUS' al tipo de dato datetime para permitir operaciones basadas en tiempo y análisis de fechas precisos. Se utilizó `errors='coerce'` para manejar posibles valores no válidos, convirtiéndolos a `NaT` (Not a Time).
4.  Detección y Manejo de Valores Atípicos: Visualizamos posibles valores atípicos en las columnas numéricas mediante boxplots. Aunque se identificaron algunos valores que podrían ser atípicos visualmente (como en 'PESO' y 'PAS_FIN'), no se aplicó una estrategia de eliminación o transformación en este paso, manteniendo los datos originales para una posterior decisión basada en el contexto del análisis.



El proceso de limpieza para "datos-hospital-b.csv"  fue algo sencillo en el analisis exploratorio vimos, principalmente porque el dataset ya presentaba una alta calidad en términos de valores nulos y duplicados. La corrección de inconsistencias en la columna 'SEXO' y la conversión de las columnas de fecha son pasos cruciales para asegurar la usabilidad del dataset en análisis posteriores, especialmente si estos involucran cálculos de edad o análisis temporales.

La decisión de no eliminar o transformar los posibles valores atípicos inmediatamente se basa en la necesidad de comprender mejor el contexto del problema. En muchos casos, los valores atípicos representan datos reales y su eliminación podría sesgar el análisis. Una estrategia más adecuada podría ser analizarlos individualmente o aplicar métodos de modelado que sean robustos a su presencia.

En general, el dataset "datos-hospital a-b.csv" ahora se encuentra en un estado mucho más limpio y preparado para las siguientes etapas del análisis de datos, con formatos estandarizados y tipos de datos correctos en las columnas clave."""

file_path = "/vinos-tintos.csv"
df = pd.read_csv(file_path)
display(df.head())

file_path = "/vinos-tintos.csv"
df_tintos = pd.read_csv(file_path)

print("Procesando Vinos Tintos")
print("Número de registros antes de eliminar duplicados:", df_tintos.shape[0])

# Verificar y eliminar duplicados
duplicados_tintos = df_tintos[df_tintos.duplicated()]
print("Número de registros duplicados encontrados:", duplicados_tintos.shape[0])

if duplicados_tintos.shape[0] > 0:
    df_tintos_cleaned = df_tintos.drop_duplicates()
    print("Número de registros después de eliminar duplicados:", df_tintos_cleaned.shape[0])
    print("Primeras filas del dataset de vinos tintos después de la limpieza:")
    display(df_tintos_cleaned.head())
else:
    df_tintos_cleaned = df_tintos.copy()
    print("No se encontraron registros duplicados en el conjunto de datos de vinos tintos.")
    print("Primeras filas del dataset de vinos tintos (sin cambios por duplicados):")
    display(df_tintos_cleaned.head())

duplicados = df[df.duplicated()]
print("Número de registros duplicados:", duplicados.shape[0])
if duplicados.shape[0] > 0:
    print("Ejemplos de registros duplicados:")
    display(duplicados.head())
else:
    print("No se encontraron registros duplicados en el conjunto de datos.")

print("Tipos de datos de las columnas en el dataset de vinos tintos:")
print(df_tintos_cleaned.dtypes)

# Seleccionar columnas numéricas para la detección de outliers en el dataset de vinos tintos
df_tintos_numeric = df_tintos_cleaned.select_dtypes(include=np.number)

# Crear boxplots para cada columna numérica
plt.figure(figsize=(15, 10))
df_tintos_numeric.boxplot()
plt.title('Boxplots de Variables Numéricas (Vinos Tintos) para Detección de Outliers')
plt.ylabel('Valor')
plt.xticks(rotation=25)
plt.grid(True)
plt.show()



file_path = "/vinos-blancos.xlsx"
df_blancos = pd.read_excel(file_path)
display(df_blancos.head())

print("Procesando Vinos Blancos")
print("Número de registros antes de eliminar duplicados:", df_blancos.shape[0])

# Verificar y eliminar duplicados
duplicados_blancos = df_blancos[df_blancos.duplicated()]
print("Número de registros duplicados encontrados:", duplicados_blancos.shape[0])

if duplicados_blancos.shape[0] > 0:
    df_blancos_cleaned = df_blancos.drop_duplicates()
    print("Número de registros después de eliminar duplicados:", df_blancos_cleaned.shape[0])
    print("\nPrimeras filas del dataset de vinos blancos después de la limpieza:")
    display(df_blancos_cleaned.head())
else:
    df_blancos_cleaned = df_blancos.copy()
    print("No se encontraron registros duplicados en el conjunto de datos de vinos blancos.")
    print("\nPrimeras filas del dataset de vinos blancos (sin cambios por duplicados):")
    display(df_blancos_cleaned.head())

print("Tipos de datos de las columnas en el dataset de vinos blancos:")
print(df_blancos_cleaned.dtypes)

# Seleccionar columnas numéricas
df_blancos_numeric = df_blancos_cleaned.select_dtypes(include=np.number)

# Crear gráficos de barras para cada columna numérica
for column in df_blancos_numeric.columns:
    plt.figure(figsize=(10, 5))
    df_blancos_numeric[column].plot(kind='bar')
    plt.title(f'Gráfico de Barras de {column} (Vinos Blancos)')
    plt.ylabel('Valor')
    plt.xlabel('Índice del Registro')
    plt.grid(axis='y')
    plt.show()

"""Evaluación del Proceso General:

El proceso de limpieza fue sistemático y cubrió los puntos clave que mencionaste en tu solicitud inicial. La adaptabilidad a diferentes formatos de archivo (.xls, .csv) y las verificaciones de calidad de datos (nulos, duplicados, tipos de datos) se realizaron de manera efectiva.

La principal diferencia en los procesos entre los datasets del hospital y los de vinos radicó en la naturaleza de las inconsistencias. Mientras que en los datos del hospital abordamos estandarización de valores categóricos y conversión de fechas, en los datos de vinos la atención se centró más en la estructura numérica de las variables.

El uso de visualizaciones como boxplots fue útil para la detección de atípicos, cumpliendo con el requisito de visualización. La justificación para no aplicar inmediatamente estrategias de manejo de atípicos (eliminación y transformación) se basó en la necesidad de un análisis más profundo y contexto del dominio, lo cual es una buena práctica en la limpieza de datos.

En general, el proceso fue sólido para identificar y abordar los problemas de calidad de datos comunes, dejando los pasos que requieren más conocimiento del dominio o decisiones basadas en el objetivo final del análisis para etapas posteriores. Los datasets resultantes están en un estado mucho más preparado para análisis exploratorio aparte encontramos 3961 datos duplicados luego volvimos a verficar si habian datos duplicados quedaron en 0
"""